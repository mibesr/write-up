\section{Introduction}

\comment{ (2 pages) The thought flows: \begin{itemize} \item Faceted search is
getting popular in applications.  \item Ordinary setup failed to address
relevnce in the presentation of discovered facets.  \item Our idea to employ an
simple extension of the standard language modeling and applying it to the
scenario.  \item A little bit about previous/similar efforts.  \item Highlight
contributions.  \item Introduce the outline. \end{itemize} }

Over the past few years, faceted search has started playing an essential role
in exploratory task for digital library applicatiosn.  While being recognized
as an eminent information seeking tool, usability of faceted search remains an
open issue for researchers to work on.  Conventional faceted search presents
the discovered facets based on the number of counts.  This type of arrangement
fails to take relevance into consideration. 

Conventional faceted search can be seen as an two-phase extension of the
ordinary retrieval task.  In the first phase, the retrieval engine collects a
set of documents that are relevant to the query $Q$, assigns each document a
\emph{relevance score}, and returns the set back to the frontend.  Different
system might implement this stage in slightly different way, but normally we
could expect that by the end of this stage, we will be able to obtain a set of
document-score pairs, i.e.,$D_Q = \{ d | d \in D \}$.  This set is called the
\emph{result set}.  Let us assume that the facet set assoicated with a document
$d$ is denoted as $F_d \subset F$.  Now, in the second phrase, the system scans
through the entire result set and put together a set of facets $F_Q$.  The
simplest and probably the most commonly-seen approach is through set union,
i.e., $F_Q = \cup_{d \in D_Q} F_d$.  

Another interesting aspect that we discovered in the development of testbed
systems is that users tend to follow top-ranked facets.  According to our
internal study on the click-through log, the average click-through rank of the
facets falls in the top 20\% of the presented facets.  This fact also motivates
our research toward integration of relevance into faceted interface, in which
users could gain better insight of the content even though they do not explicit
go through every facet that we discovered.

In this work, we attempt to improve faceted search usability by introducing the
notion of relevance into the facet interface.  

%--------------------------------------------------
% Query-dependent faceted navigation did not received as much attention until
% recently, when faceted search interface started being recognized as an eminent
% information seeking tool in various kinds of digital library applications.  In
% the technical respect, the major obstacle for employing this technique in a
% real-world setup lies in the time complexity.  When facing a user query, the
% retrieval system that supports such a function would first put together a
% complete list of document identifiers that covers all the documents that match
% the query from the inverted index, explicitly scan through the forward index
% that maps a document to a set of associated facets for all the matched
% document, collect facet captions and counts, and finally return all these
% information back to the user.  Since the operation involves a set of results
% that is only known in run-time, its complexity is largely $O(K \times |R|)$ in
% time, where $K$ denotes an average number of facets being associated with a
% document, and $|R|$ denotes the size of the query result set.  
% 
% Employing a relational database backend is the easiest, while also the most
% practical, approach towards construction of this function.  The document
% contents are usually stored in one table and associated facet records in
% another, and then implement the search function with a plain SQL text match
% against the documents.  The set of facets associated with all the matched
% document and their corresponding counts of occurrences can be pulled out as
% easily with a join or SQL aggregation functions such as {\tt GROUP BY} and {\tt
% COUNT}.  The downside of this approach is we compromise retrieval quality in
% favor of ease of extra efforts because most of the modern relational database
% systems do not really support fuzzy-match in text fields, least to say the
% state-of-the-art document retrieval models.  A proposed workaround for this is
% to host retrieval engine and document representation somewhere else and access
% the RDBMS only for aggregating facet counts.  This could be done fairly easily
% by using the retrieved document identifiers to explicitly form a (probably very
% long) {\tt SELECT} query.  Unfortunately, this solution might not scale well,
% since serving data in an RDBMS way, on the other hand, can obstruct the
% interaction to the retrieval model and degrade the performance.
%-------------------------------------------------- 

%--------------------------------------------------
% 
% This work is motivated by the aforementioned observation... \comment{ \it Motivations:
% speed-up and support for state-of-the-art retrieval functions.  }
% 
% Our contribution lies in ... \comment{ Contributions }
% 
% Along with the wide acceptance of faceted search interface in the digital
% library community, an noticable amount of efforts has recently been put on
% usability tests and discovery of more advanced usage.  Previous efforts
% include... \comment{ Related work }
%-------------------------------------------------- 

\section{Model}

%--------------------------------------------------
% \comment{(3 pages) This section should cover the following three technical aspects of the
% model: \begin{itemize} \item Mathematical framework stemmed from the basic language modeling
% approach; \item The required modification to the index structure; \item complexity analysis. \end{itemize}}
%-------------------------------------------------- 

We recast the faceted search as a language modeling framework, in which we take
a document-centric view and assume that each document $d_j$ is associated with
two distributions $\Pr(t_i|d_j)$ and $\Pr(f_k|d_j)$.  The first distribution
$\Pr(t_i|d_j)$ is a multinomial distribution over the term domain $T$ that
governs the generation of term occurrences in document $d_j$, and the second
distribution $\Pr(f_k|d_j)$ is a multinomial distribution over the facet domain
$F$ that offers similar use to the generatio n of facets.  The generative
process can be described as follows: \footnote{A formal way to model the
generation of the corresponding set size $|T_j|$ and $|F_j|$ is through Poisson
processes.  Here, however, for simplicity we assume these numbers are
pre-determined for each document $d_j$.} 

\begin{itemize} \item For each document $d_j$, \begin{enumerate} \item For $i
\in \{ 1, \ldots, |T_j| \}$, $t_i \sim \textrm{Mult}(d_j)$ \item For $k \in \{
  1, \ldots, |F_j| \}$, $f_k \sim \textrm{Mult}(d_j)$ \end{enumerate}
  \end{itemize}

The task of retrieving relevant facets can be modeled as:

\begin{figure}[ht!]
  \centering
  \psset{xunit=10mm,yunit=10mm}
  \begin{pspicture}(0,0)(8,2)%\showgrid  % Uncomment to show grid!
    \SpecialCoor  % (a|b) means x-coord from 'a' and y-coord from 'b'.
    \psset{arrowscale=1.5}

    \rput(4.0,1.0){\GM@parameter{d}}\GM@label[angle=45]{d}{$d_j$}
    \rput(1.0,1.0){\GM@node{t}}\GM@label[angle=45]{t}{$t_i$}
    \rput(0.25, 0.25){\GM@plate{1.5}{1.5}{$|T_j|$}}
    \rput(7.0,1.0){\GM@node{f}}\GM@label[angle=45]{f}{$f_k$}
    \rput(6.25, 0.25){\GM@plate{1.5}{1.5}{$|F_j|$}}

    \ncline[arrows=->]{d}{t}
    \ncline[arrows=->]{d}{f}
  \end{pspicture}

  \caption{A document-centric generative model.  The document $d_j$ generates a
  set of terms $T_j$ in which each term is represented as $t_i$; meanwhile, the
  document is associated with a set of facets $F_j$ in which each facet is
  denoted as $f_k$.}
  \label{f:model}
\end{figure}

\begin{eqnarray}
  \Pr(f|Q) &=& \sum_{d \in D} \Pr(f|d,Q) \Pr(d|Q) \nonumber\\
  &=& \sum_{d \in D} \Pr(f|d) \Pr(d|Q) \nonumber\\
  &\propto& \sum_{d \in D} \Pr(f|d) \Pr(Q|d) \label{eq:pr(f|Q)}\\
  && \nonumber\\
  \Pr(Q|d) &=& \prod_{q \in Q} \Pr(q|d) \nonumber\\
  &\propto& \sum_{q \in Q} \log \Pr(q|d) \label{eq:pr(Q|d)}
\end{eqnarray}

\paragraph{Estimation for $\Pr(Q|d)$.}

\begin{eqnarray}
  \Pr(Q|d) &=& \exp \left(\sum_{q \in Q} \log \Pr(q|d) \right) \nonumber\\
  &=& \exp \left(\sum_{q \in Q} \log \left[ \frac{f_{q,d} + \mu f_q / |C|}{|d| + \mu} \right] \right) \nonumber\\
  &=& \exp \left(\sum_{q \in Q} \log \left( f_{q,d} + \mu f_q / |C| \right) - |Q| \log \left( |d| + \mu \right) \right) \nonumber\\
  &=& K \times \exp \left( \sum_{q \in Q \cap d} \log \left( \frac{f_{q,d} |C|}{\mu f_q} + 1 \right) 
  - |Q| \log \left( |d| + \mu \right) \right) 
\end{eqnarray}
where \[ K = \exp \left( |Q| \log \mu - |Q| \log |C| + \sum_{q \in Q} \log f_q \right). \]

\paragraph{Estimation for $\Pr(f|Q)$.}

\begin{eqnarray}
  \Pr(f|Q) &\propto& \sum_{d \in D} \Pr(f|d) \Pr(Q|d) \nonumber\\
  &=& \sum_{d \in \{d'|f \in d\}} \frac{c_{f,d}}{\mu_2 + c_d} \Pr(Q|d) + \sum_{d \in D} \frac{\mu_2 c_f / |F|}{\mu_2 + c_d} \Pr(Q|d)
\end{eqnarray}

Let $c_{f,d} = 1$ and $K_f = \mu_2 c_f/|F|$.

\begin{eqnarray}
  \Pr(f|q) &\propto& \sum_{d \in \{d'|f \in d\}} \frac{1}{\mu_2 + c_d} \Pr(Q|d) \\
  && \quad + K_f \sum_{d \in D} \frac{1}{\mu_2 + c_d} \Pr(Q|d)
\end{eqnarray}

\subsection{Algorithm}

%--------------------------------------------------
% \section{Approximate Query-Dependent Faceted Navigation}
% 
% \subsection{Index Structure} 
% The guideline in development of approximate facet count retrieval algorithm is
% to prevent access to the document forward index.  \comment{ The key for speed-up
% is to not scanning through the index }
% 
% One way to do it is to build up a term-facet inverted index.  We prefetch all
% the associated facets and corresponding counts in the relevant set and store
% all these information in the inverted structure.  The example.... \comment{ The
% idea of building the inverted index }
% 
% Practical concerns for such arrangement incluce ... \comment{ Here we address
% the trick that we use to speed up the computation.  We might also want to
% address other possibilities for achieving the same goal. }
% 
% \subsection{Aggregation Methods}
% 
% An straightforward idea for aggregating the results for all the query terms is
% to average the facet counts. \comment{ Average }
% 
% By viewing the facet counts ... \comment{ Maximum }
% 
% \comment{ Interpolation }
% 
% \subsection{Error estimation} 
% We recast the problem as rank-correlation optimization problem.  \comment{ Here we
% elaborate on developing faceted navigation in conventional approach, and we
% formulate the approximated version of the problem. }
% 
% \comment{ Here we address how likely we are about to
% introduce errors, and how large it could be. }
%-------------------------------------------------- 

\section{Evaluation}

\comment{(1 1/2 pages) We briefly introduce the way we evaluate the performance.  Performance
could be measured in terms of precision/recall (or accuracy only) by letting a
domain expert explicitly annotating the entire query set.  Then we introduce
the benchmark (Dansin dataset and the corresponding query set).}

We use Dansin dataset as our reference corpus.  The dataset contains N,NNN
documents.  Each documents comes in as an XML file that contains fulltext
transcripts, metadata items, and associated human-selected facets.  The average
number of facets associated with a document in Dansin dataset is N.NNN.  There
are two types of facets involved in this study: contents of certain selected
fields, and hand-labeled concepts.  Hand-labeled concepts, in fact, are created
by a group of annotators.  All these information are already made available in
the metadata of the documents.

The major challenge for conducting such an evaluation task is to devise a set
of query topics.  Since most of the digital libraries are very much
domain-specific, it is difficult to find a standard reference query topics that
could be used across studies.  The approach that we took was to look for
session query logs in the relevant domain.  In the end, we took a copy of the
query log in 2009 from the DARC system which was developed and maintained by
Digital Archives Research Center.  

We preprocessed the log and remove all the entries were follow-up clicks on the
relevant facets and leave only original user queries in the file.  The total
amount of unique identifiable query topics is NN,NNN.  300 topics were randomly
drawn from the population and gone through human inspection.  Since the DARC
system is an aggregation service of a number of underlying database system, it
is possible that the query log contains irrelevant topics to the Dansin set.
Finally 151 topics were selected and used in the evaluation.

\comment{ Here we describe our effort for a number of proposed models.  The
following items shall be addressed in order: Experimental runs: baseline,
fuzzy-min, interpolation, and top-k.  }

\section{Results}

\comment{ (2 pages) Here we present the experimental results.  Precision/recall
plot would be great.  Another way to evaluate is the Spearman's rho by asking
the domain experts to rank the facets explicitly and the annotations should
serve as gold standard for all the models to compare with. }

\section{Related Work}

\comment{ (Half a page) Introduce the related work in the following domains: (1) language modeling, (2) faceted search, and (3) expert finding. }

Among the previous research, one closest effort to our approach is found in the
domain of expert finding.  Balog et al. \cite{balog2009language} proposed a
language modeling framework with the similar rationale to extend the use of
language model to the underlying co-occurrence statistics by exploiting
person-term and person-document links.  The difference between our contribution
and theirs lies, not only in the application domain, but in the way relevance
is introduced into the model against the facets and the explicit scoring
formula for online query support.
 
\section{Discussion and Concluding Remarks}

\comment{ (2 pages) Several issues that should be brought up here: (1) Why
relevance is also important in exploratory task, (2) the possible opposite
formulation of the model for novelty discovery (i.e., $\Pr(Q|f)$), and (3)
possible future directions.  Also draw conclusion here. }

%--------------------------------------------------
% \subsection{Measures}
% 
% The evaluation measures we used to assess the similarity between the original
% facets and the approximation is as follows.
% 
% \begin{enumerate}
% \item $p$-norm: The measure evaluates the distance between two vectors.  In this work, we use only L1 and L2 norms.
% \item Jaccard coefficient: The measure assesses the similarity between two sets, defined by
% \[ J(X,Y) = \frac{X \cap Y}{X \cup Y}. \]
% \item Spearman's rho: The measure assesses the rank correlation between two rank lists, defined by
% \[ \rho(X,Y) = 1. \]
% \end{enumerate}
% 
% \subsection{Results}
% Here we demonstrate the experimental results and give a brief analysis.
%-------------------------------------------------- 

\section{Acknowledgments}

\comment{ A few names shall be mentioned here, including NTU DTRAP and its corresponding funding project, and Po-Yu Chen. }

